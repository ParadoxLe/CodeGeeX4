import json
import os
import jsonlines
from typing import List, Dict
import warnings
import textwrap
import re

# å±è”½HuggingFaceçš„FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning)
# è®¾ç½®é•œåƒæºï¼ˆéœ€åœ¨åŠ è½½æ¨¡å‹å‰æ‰§è¡Œï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from human_eval.evaluation import evaluate_functional_correctness  # å®˜æ–¹è¯„ä¼°å‡½æ•°

# ... å…¶ä»– import ...
import jsonlines
from code_refiner import CodeRefiner  # <--- æ–°å¢
from code_gnn_reranker import GNNReranker  # <--- ä¹‹å‰å†™çš„ GNN

# ================= é…ç½®å‚æ•° =================
model_path = "zai-org/codegeex4-all-9b"
dataset_name = "evalplus/humanevalplus"
output_dir = "./humaneval_results"  # ç»Ÿä¸€ä¿å­˜æ–‡ä»¶å¤¹
output_file = "humaneval_candidates_with_problem.jsonl"  # å¸¦é—®é¢˜æ ‡æ³¨çš„å€™é€‰è§£æ–‡ä»¶
k_values = [1, 10, 100]  # è¦è®¡ç®—çš„ pass@k
max_new_tokens = 1024  # æ¯ä¸ªå€™é€‰è§£çš„æœ€å¤§é•¿åº¦
temperature = 0.2  # é‡‡æ ·æ¸©åº¦ï¼ˆç”Ÿæˆå¤šä¸ªè§£éœ€è¦å¼€å¯é‡‡æ ·ï¼‰
top_p = 0.95  # æ ¸é‡‡æ ·
batch_size = 8  # æ‰¹é‡ç”Ÿæˆï¼ˆæ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼‰

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨ï¼‰
os.makedirs(output_dir, exist_ok=True)
final_output_path = os.path.join(output_dir, output_file)  # æœ€ç»ˆä¿å­˜è·¯å¾„


# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
def load_model_and_tokenizer(model_path: str):
    """åŠ è½½æ¨¡å‹å’ŒTokenizer"""
    print(f"Loading model: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, trust_remote_code=True, padding_side="left"  # å·¦paddingæ›´é€‚åˆLLMç”Ÿæˆ
    )
    # è¡¥å……pad_tokenï¼ˆå¦‚æœæ¨¡å‹æ²¡æœ‰ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype="auto"  # è‡ªåŠ¨é€‰æ‹©é«˜ç²¾åº¦ dtypeï¼ˆå¦‚FP16/FP8ï¼‰
    )
    model.eval()  # æ¨ç†æ¨¡å¼
    return tokenizer, model


def generate_candidates(
        tokenizer, model, prompt: str, num_candidates: int, batch_size: int
) -> List[str]:
    """ç”ŸæˆæŒ‡å®šæ•°é‡çš„å€™é€‰è§£ï¼ˆä¿®å¤æ‰¹é‡ç”Ÿæˆæº¢å‡ºbugï¼‰"""
    candidates = []
    remaining = num_candidates

    # æ„å»ºå¯¹è¯æ¨¡æ¿
    messages = [{'role': 'user', 'content': f"""Write a solution to the following problem:
```python
{prompt}
```"""}]
    inputs = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt"
    ).to(model.device)

    while remaining > 0:
        current_batch_size = min(batch_size, remaining)

        # ä»…ç”¨ num_return_sequences æ§åˆ¶æ‰¹é‡ç”Ÿæˆæ•°é‡
        outputs = model.generate(
            inputs,  # å•ä¸ªè¾“å…¥ï¼Œä¸é‡å¤
            max_new_tokens=max_new_tokens,
            do_sample=True,  # å¿…é¡»å¼€å¯é‡‡æ ·æ‰èƒ½ç”Ÿæˆä¸åŒè§£
            temperature=temperature,
            top_p=top_p,
            num_return_sequences=current_batch_size,  # 1ä¸ªè¾“å…¥ç”Ÿæˆ current_batch_size ä¸ªè§£
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            repetition_penalty=1.1  # å‡è½»é‡å¤ç”Ÿæˆ
        )

        # è§£ç å¹¶è¿‡æ»¤ç‰¹æ®Štoken
        for output in outputs:
            candidate = tokenizer.decode(
                output[len(inputs[0]):],  # è·³è¿‡è¾“å…¥éƒ¨åˆ†
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            # ç®€å•æ¸…ç†ï¼šåªä¿ç•™å‡½æ•°å®šä¹‰éƒ¨åˆ†ï¼ˆé¿å…å¤šä½™æ–‡æœ¬ï¼‰
            candidate = clean_candidate(candidate)
            if candidate:
                candidates.append(candidate)

        remaining -= current_batch_size
        print(f"Generated {len(candidates)}/{num_candidates} candidates")

    return candidates[:num_candidates]  # ç¡®ä¿æœ€ç»ˆåªè¿”å›ç›®æ ‡æ•°é‡


def format_oneliner(code: str) -> str:
    """
    ä¸“é—¨ä¿®å¤è¢«å‹ç¼©æˆä¸€è¡Œçš„ä»£ç ã€‚
    ä¾‹å¦‚: "for i in ...: if ...: return True return False"
    """
    # 1. æ ¸å¿ƒé€»è¾‘ï¼šåœ¨å†’å· : åé¢å¦‚æœç´§è·Ÿå…³é”®å­— (for, if, return ç­‰)ï¼Œæ’å…¥æ¢è¡Œ
    # Group 1: :
    # Group 2: å…³é”®å­—
    pattern_colon = r'(:)\s+(for|while|if|elif|else|return|try|except|with|def|class)'
    code = re.sub(pattern_colon, r'\1\n\2', code)

    # 2. å¤„ç† return/break/continue è¿™ç§ç»“æŸæ€§è¯­å¥
    # å¦‚æœå®ƒä»¬å‡ºç°åœ¨è¡Œä¸­é—´ï¼ˆå‰é¢æœ‰ç©ºæ ¼ï¼‰ï¼Œè¯´æ˜éœ€è¦æ¢è¡Œ
    # ä¾‹å¦‚ "... return True return False" -> "... return True\nreturn False"
    pattern_statement = r'\s+(return|break|continue|yield|raise)'
    code = re.sub(pattern_statement, r'\n\1', code)

    # 3. é‡æ–°æ„å»ºç¼©è¿›
    # ç®€å•çš„ç¼©è¿›çŠ¶æ€æœº
    lines = code.split('\n')
    formatted_lines = []
    indent_level = 0
    indent_unit = "    "  # 4ä¸ªç©ºæ ¼

    for i, line in enumerate(lines):
        line = line.strip()
        if not line: continue

        # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœæ˜¯ else/elifï¼Œå…ˆå›é€€ä¸€çº§
        if line.startswith(("else", "elif", "except", "finally")):
            indent_level = max(0, indent_level - 1)

        # ç‰¹æ®Šä¿®æ­£ï¼šå¦‚æœä¸Šä¸€è¡Œæ˜¯ returnï¼Œä¸”å½“å‰è¡Œä¹Ÿæ˜¯ returnï¼Œ
        # æˆ–è€…å½“å‰è¡Œæ˜¯æœ€åä¸€è¡Œä¸”æ˜¯ returnï¼Œé€šå¸¸æ„å‘³ç€å®ƒå±äºå¤–å±‚
        # è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ HumanEval çš„ç‰¹å®šä¼˜åŒ–
        if line.startswith("return") and i > 0 and lines[i - 1].strip().startswith("return"):
            # è¿ç»­ä¸¤ä¸ª returnï¼Œç¬¬äºŒä¸ªè‚¯å®šåœ¨å¤–å±‚ï¼Œå¼ºåˆ¶å›é€€ç¼©è¿›
            indent_level = max(0, indent_level - 2)  # å›é€€ä¸¤çº§é€šå¸¸æ¯”è¾ƒå®‰å…¨

        # æ·»åŠ ç¼©è¿›
        formatted_lines.append((indent_unit * indent_level) + line)

        # è®¡ç®—ä¸‹ä¸€è¡Œçš„ç¼©è¿›
        if line.endswith(":"):
            indent_level += 1
        elif line.startswith("return ") or line == "return":
            # é‡åˆ° returnï¼Œç†è®ºä¸Šç¼©è¿›åº”è¯¥å‡å°‘ï¼Œä½†åœ¨ Python è¯­æ³•è§£æä¸­å¾ˆéš¾ç¡®å®šå‡å°‘å‡ çº§
            # è¿™é‡Œä¸åšè‡ªåŠ¨å‡å°‘ï¼Œä¾é ä¸Šé¢çš„ "è¿ç»­ return" è§„åˆ™æ¥ä¿®è¡¥
            pass

    return "\n".join(formatted_lines)


def clean_candidate(candidate: str) -> str:
    # 1. é¢„å¤„ç†ï¼šå¦‚æœåŒ…å« ```ï¼Œå»æ‰å¤–å±‚
    if "```" in candidate:
        candidate = candidate.split("```")[-2] if len(candidate.split("```")) > 2 else candidate
        if candidate.strip().startswith("python"):
            candidate = candidate.strip()[6:]

    # 2. å»æ‰ Docstring
    if '"""' in candidate:
        parts = candidate.split('"""')
        if len(parts) >= 3:
            candidate = parts[-1]
    elif "'''" in candidate:
        parts = candidate.split("'''")
        if len(parts) >= 3:
            candidate = parts[-1]

    candidate = candidate.strip()

    # 3. ã€å…³é”®ä¿®å¤ã€‘æ£€æµ‹æ˜¯å¦ä¸ºå•è¡Œä»£ç 
    # å¦‚æœä»£ç é•¿åº¦å¤§äº50å­—ç¬¦ï¼Œä¸”æ²¡æœ‰æ¢è¡Œç¬¦ï¼Œæˆ–è€…åŒ…å« "for " ä¸”æ²¡æœ‰æ¢è¡Œ
    if "\n" not in candidate and len(candidate) > 20:
        return format_oneliner(candidate)

    # 4. å¦‚æœæœ¬èº«å°±æ˜¯å¤šè¡Œï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼Œåšæ ‡å‡† Dedent
    lines = candidate.split("\n")
    # ... (æ­¤å¤„çœç•¥å¸¸è§„å¤šè¡Œå¤„ç†é€»è¾‘ï¼Œå› ä¸ºä½ çš„é—®é¢˜ä¸»è¦æ˜¯å•è¡Œ) ...
    # å¦‚æœä½ ç¡®è®¤è¾“å…¥å¤§éƒ¨åˆ†æ˜¯å•è¡Œï¼Œç›´æ¥è¿”å› format_oneliner(candidate) ä¹Ÿè¡Œ
    return format_oneliner(candidate)


def save_task_candidates(task_candidates: List[Dict], output_path: str):
    """è¿½åŠ ä¿å­˜å•ä¸ªä»»åŠ¡çš„å€™é€‰è§£åˆ°JSONLæ–‡ä»¶ï¼ˆå¸¦é—®é¢˜æ ‡æ³¨ï¼‰"""
    # ä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼ˆaï¼‰ï¼Œé¿å…è¦†ç›–å·²ä¿å­˜çš„å†…å®¹
    with jsonlines.open(output_path, "a") as f:
        f.write_all(task_candidates)
    print(f" Task candidates saved to: {output_path} (added {len(task_candidates)} candidates)\n")


def calculate_pass_at_k(candidates_path: str, k_values: List[int]) -> Dict:
    """ä½¿ç”¨å®˜æ–¹å·¥å…·è®¡ç®—pass@k"""
    print(f"\nCalculating pass@k for k={k_values}...")
    results = evaluate_functional_correctness(
        candidates_path,
        k=k_values,
        timeout=30  # æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    )
    return results


# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    #  æ–°å¢ï¼šåˆ¤æ–­å€™é€‰è§£æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(final_output_path):
        print(f" å·²æ‰¾åˆ°å€™é€‰è§£æ–‡ä»¶ï¼š{final_output_path}")
        print("ç›´æ¥è·³è¿‡ç”Ÿæˆæ­¥éª¤ï¼Œå¼€å§‹è¯„ä¼°...\n")
        # åŠ è½½æ•°æ®é›†ï¼ˆä»…ç”¨äºè®¡ç®—ç»“æœæ—¶æ˜¾ç¤ºä»»åŠ¡æ•°é‡ï¼Œä¸ç”¨åŠ è½½æ¨¡å‹ï¼‰
        dataset = load_dataset(dataset_name, split="test")
        total_tasks = len(dataset)

    else:
        #  æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼Œæ‰æ‰§è¡ŒåŸæ¥çš„ã€ŒåŠ è½½æ¨¡å‹+ç”Ÿæˆå€™é€‰è§£ã€é€»è¾‘
        # 1. åŠ è½½æ¨¡å‹ã€Tokenizerå’Œæ•°æ®é›†
        tokenizer, model = load_model_and_tokenizer(model_path)
        # åˆå§‹åŒ– Refiner (ä¼ å…¥åˆšåŠ è½½çš„æ¨¡å‹ï¼Œä¸éœ€è¦é‡æ–°åŠ è½½)
        refiner = CodeRefiner(tokenizer, model)  # <--- æ–°å¢
        dataset = load_dataset(dataset_name, split="test")  # HumanEval+æµ‹è¯•é›†
        print(f"Loaded dataset with {len(dataset)} tasks\n")
        # 2. ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡çš„å€™é€‰è§£ï¼ˆæ¯ä¸ªä»»åŠ¡ç”Ÿæˆåç«‹å³ä¿å­˜ï¼Œå¸¦é—®é¢˜æ ‡æ³¨ï¼‰
        max_candidates_per_task = max(k_values)
        total_tasks = len(dataset)

        #  åˆå§‹åŒ– GNN é‡æ’åºå™¨
        # æ³¨æ„ï¼šå¦‚æœä½ è¿˜æ²¡æœ‰è®­ç»ƒå¥½çš„æƒé‡æ–‡ä»¶ï¼Œè¿™é‡Œå®ƒä¼šä½¿ç”¨éšæœºæƒé‡è¿è¡Œ
        # è¿™ä¸»è¦ç”¨äºæµ‹è¯•æµç¨‹æ˜¯å¦è·‘é€šã€‚çœŸæ­£æå‡æ•ˆæœéœ€è¦ 'gnn_model.pth'
        gnn_reranker = GNNReranker(model_path="gnn_model.pth")

        # 2. ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡çš„å€™é€‰è§£...
        max_candidates_per_task = max(k_values)

        for idx, data in enumerate(dataset):
            # æå–å½“å‰ä»»åŠ¡çš„å…³é”®ä¿¡æ¯ï¼ˆç”¨äºæ ‡æ³¨ï¼‰
            task_id = f"HumanEval/{idx}"
            problem_prompt = data["prompt"].strip()  # å®Œæ•´é—®é¢˜æè¿°ï¼ˆå‡½æ•°å®šä¹‰+æ³¨é‡Šï¼‰
            # æå–å‡½æ•°åä½œä¸ºä»»åŠ¡åï¼ˆæ›´æ˜“è¯»ï¼‰
            func_name = None
            for line in problem_prompt.split("\n"):
                if line.startswith("def "):
                    func_name = line.split("def ")[1].split("(")[0].strip()
                    break
            task_name = f"func_{func_name}" if func_name else f"task_{idx}"  # å¦‚ func_fibonacci

            print(f"=== Processing task {idx + 1}/{total_tasks} ===")
            print(f"Task ID: {task_id}")
            print(f"Task Name: {task_name}")
            print(f"Problem Preview:\n{problem_prompt[:300]}..." if len(
                problem_prompt) > 300 else f"Problem:\n{problem_prompt}")

            # ç”Ÿæˆå½“å‰ä»»åŠ¡çš„å€™é€‰è§£
            candidates = generate_candidates(
                tokenizer=tokenizer,
                model=model,
                prompt=problem_prompt,
                num_candidates=max_candidates_per_task,
                batch_size=batch_size
            )

            # ã€å…³é”®æ­¥éª¤ã€‘ä½¿ç”¨ GNN å¯¹å€™é€‰è§£è¿›è¡Œé‡æ’åº
            # åŸç†ï¼šè™½ç„¶ç”Ÿæˆäº† 100 ä¸ªï¼Œä½†æ’åœ¨ç¬¬ 0 ä½çš„å¯èƒ½æ˜¯é”™çš„ã€‚
            # GNN å°è¯•æŠŠâ€œé•¿å¾—åƒæ­£ç¡®ä»£ç â€çš„è§£æ’åˆ°æœ€å‰é¢ã€‚
            print(f"  > Reranking {len(candidates)} candidates with GNN...")
            candidates = gnn_reranker.rerank(candidates)

            # æ„å»ºå½“å‰ä»»åŠ¡çš„å€™é€‰è§£åˆ—è¡¨ï¼ˆå¸¦é—®é¢˜æ ‡æ³¨ï¼‰
            task_candidates = []
            for cand_idx, candidate in enumerate(candidates):
                task_candidates.append({
                    "task_id": task_id,  # æ ‡å‡†ä»»åŠ¡IDï¼ˆå¦‚ HumanEval/0ï¼‰
                    "task_name": task_name,  # æ˜“è¯»ä»»åŠ¡åï¼ˆå¦‚ func_fibonacciï¼‰
                    "problem_description": problem_prompt,  # å®Œæ•´é—®é¢˜æè¿°ï¼ˆæ–¹ä¾¿è¿½æº¯ï¼‰
                    "candidate_id": f"{task_id}_candidate_{cand_idx}",  # å”¯ä¸€å€™é€‰è§£IDï¼ˆå¦‚ HumanEval/0_candidate_5ï¼‰
                    "candidate_index": cand_idx,  # å€™é€‰è§£åœ¨å½“å‰ä»»åŠ¡ä¸­çš„åºå·ï¼ˆ0-99ï¼‰
                    "completion": candidate  # æ¨¡å‹ç”Ÿæˆçš„è§£
                })

            # é€ä»»åŠ¡è¿½åŠ ä¿å­˜ï¼ˆå¸¦é—®é¢˜æ ‡æ³¨ï¼‰
            save_task_candidates(task_candidates, final_output_path)

    # ğŸ‘‡ è¯„ä¼°é€»è¾‘ä¸å˜ï¼ˆæ— è®ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œéƒ½ä¼šæ‰§è¡Œï¼‰
    print("\n" + "=" * 60)
    print("Starting pass@k evaluation...")
    print("=" * 60)

    results = calculate_pass_at_k(final_output_path, k_values)

    # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
    print("\n" + "=" * 50)
    print("Pass@k Results (HumanEval+)")
    print("=" * 50)
    for k in k_values:
        pass_k = results[f"pass@{k}"]
        print(f"pass@{k}: {pass_k:.4f} ({int(pass_k * total_tasks)}/{total_tasks} tasks passed)")
    print("=" * 50)

    # ä¿å­˜pass@kç»“æœåˆ°åŒä¸€æ–‡ä»¶å¤¹
    results_path = os.path.join(output_dir, "pass_at_k_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\n Pass@k results saved to: {results_path}")
    print(f" å¸¦é—®é¢˜æ ‡æ³¨çš„å€™é€‰è§£æ–‡ä»¶ï¼š{final_output_path}")
    print(f" æ‰€æœ‰æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼š{output_dir}")
