import json
import os
import jsonlines
from typing import List, Dict
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)  # å±è”½HuggingFaceçš„FutureWarning
# è®¾ç½®é•œåƒæºï¼ˆéœ€åœ¨åŠ è½½æ¨¡å‹å‰æ‰§è¡Œï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from human_eval.evaluation import evaluate_functional_correctness  # å®˜æ–¹è¯„ä¼°å‡½æ•°

# ================= é…ç½®å‚æ•° =================
model_path = "zai-org/codegeex4-all-9b"
dataset_name = "evalplus/humanevalplus"
output_dir = "./humaneval_results"  # ç»Ÿä¸€ä¿å­˜æ–‡ä»¶å¤¹
output_file = "humaneval_candidates_with_problem.jsonl"  # å¸¦é—®é¢˜æ ‡æ³¨çš„å€™é€‰è§£æ–‡ä»¶
k_values = [1, 10, 100]  # è¦è®¡ç®—çš„ pass@k
max_new_tokens = 1024  # æ¯ä¸ªå€™é€‰è§£çš„æœ€å¤§é•¿åº¦
temperature = 0.2  # é‡‡æ ·æ¸©åº¦ï¼ˆç”Ÿæˆå¤šä¸ªè§£éœ€è¦å¼€å¯é‡‡æ ·ï¼‰
top_p = 0.95
batch_size = 8  # æ‰¹é‡ç”Ÿæˆï¼ˆæ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼‰

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨ï¼‰
os.makedirs(output_dir, exist_ok=True)
final_output_path = os.path.join(output_dir, output_file)  # æœ€ç»ˆä¿å­˜è·¯å¾„


# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
def load_model_and_tokenizer(model_path: str):
    """åŠ è½½æ¨¡å‹å’ŒTokenizer"""
    print(f"Loading model: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, trust_remote_code=True, padding_side="left"  # å·¦paddingæ›´é€‚åˆLLMç”Ÿæˆ
    )
    # è¡¥å……pad_tokenï¼ˆå¦‚æœæ¨¡å‹æ²¡æœ‰ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype="auto"  # è‡ªåŠ¨é€‰æ‹©é«˜ç²¾åº¦ dtypeï¼ˆå¦‚FP16/FP8ï¼‰
    )
    model.eval()  # æ¨ç†æ¨¡å¼
    return tokenizer, model


def generate_candidates(
        tokenizer, model, prompt: str, num_candidates: int, batch_size: int
) -> List[str]:
    """ç”ŸæˆæŒ‡å®šæ•°é‡çš„å€™é€‰è§£ï¼ˆä¿®å¤æ‰¹é‡ç”Ÿæˆæº¢å‡ºbugï¼‰"""
    candidates = []
    remaining = num_candidates

    # æ„å»ºå¯¹è¯æ¨¡æ¿
    messages = [{'role': 'user', 'content': f"""Write a solution to the following problem:
```python
{prompt}
```"""}]
    inputs = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt"
    ).to(model.device)

    while remaining > 0:
        current_batch_size = min(batch_size, remaining)

        # ä¿®å¤ï¼šç§»é™¤ inputs.repeatï¼Œä»…ç”¨ num_return_sequences æ§åˆ¶æ‰¹é‡ç”Ÿæˆæ•°é‡
        outputs = model.generate(
            inputs,  # å•ä¸ªè¾“å…¥ï¼Œä¸é‡å¤
            max_new_tokens=max_new_tokens,
            do_sample=True,  # å¿…é¡»å¼€å¯é‡‡æ ·æ‰èƒ½ç”Ÿæˆä¸åŒè§£
            temperature=temperature,
            top_p=top_p,
            num_return_sequences=current_batch_size,  # 1ä¸ªè¾“å…¥ç”Ÿæˆ current_batch_size ä¸ªè§£
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            repetition_penalty=1.1  # å‡è½»é‡å¤ç”Ÿæˆ
        )

        # è§£ç å¹¶è¿‡æ»¤ç‰¹æ®Štoken
        for output in outputs:
            candidate = tokenizer.decode(
                output[len(inputs[0]):],  # è·³è¿‡è¾“å…¥éƒ¨åˆ†
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            # ç®€å•æ¸…ç†ï¼šåªä¿ç•™å‡½æ•°å®šä¹‰éƒ¨åˆ†ï¼ˆé¿å…å¤šä½™æ–‡æœ¬ï¼‰
            candidate = clean_candidate(candidate)
            if candidate:
                candidates.append(candidate)

        remaining -= current_batch_size
        print(f"Generated {len(candidates)}/{num_candidates} candidates")

    return candidates[:num_candidates]  # ç¡®ä¿æœ€ç»ˆåªè¿”å›ç›®æ ‡æ•°é‡


def clean_candidate(candidate: str) -> str:
    """æ¸…ç†å€™é€‰è§£ï¼šæå–å‡½æ•°å®šä¹‰ï¼Œå»é™¤å¤šä½™å†…å®¹"""
    # æ‰¾åˆ°å‡½æ•°å®šä¹‰çš„å¼€å§‹ï¼ˆdef æˆ– classï¼‰
    lines = [line.strip() for line in candidate.split("\n") if line.strip()]
    func_lines = []
    in_func = False
    indent_level = 0

    for line in lines:
        if line.startswith(("def ", "class ")):
            in_func = True
            func_lines.append(line)
            # è®¡ç®—ç¼©è¿›çº§åˆ«ï¼ˆå‡è®¾ç”¨4ä¸ªç©ºæ ¼ï¼‰
            indent_level = len(line) - len(line.lstrip())
        elif in_func:
            # å¦‚æœå½“å‰è¡Œç¼©è¿› >= å‡½æ•°å®šä¹‰çš„ç¼©è¿›ï¼Œå±äºå‡½æ•°å†…å®¹
            current_indent = len(line) - len(line.lstrip())
            if current_indent >= indent_level or line.startswith(("return", "if", "for", "while", "with", "try")):
                func_lines.append(line)
            else:
                # ç¼©è¿›å˜å°ï¼Œè¯´æ˜å‡½æ•°ç»“æŸ
                break

    return "\n".join(func_lines) if func_lines else candidate


def save_task_candidates(task_candidates: List[Dict], output_path: str):
    """è¿½åŠ ä¿å­˜å•ä¸ªä»»åŠ¡çš„å€™é€‰è§£åˆ°JSONLæ–‡ä»¶ï¼ˆå¸¦é—®é¢˜æ ‡æ³¨ï¼‰"""
    # ä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼ˆaï¼‰ï¼Œé¿å…è¦†ç›–å·²ä¿å­˜çš„å†…å®¹
    with jsonlines.open(output_path, "a") as f:
        f.write_all(task_candidates)
    print(f" Task candidates saved to: {output_path} (added {len(task_candidates)} candidates)\n")


def calculate_pass_at_k(candidates_path: str, k_values: List[int]) -> Dict:
    """ä½¿ç”¨å®˜æ–¹å·¥å…·è®¡ç®—pass@k"""
    print(f"\nCalculating pass@k for k={k_values}...")
    results = evaluate_functional_correctness(
        candidates_path,
        k=k_values,
        timeout=30  # æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    )
    return results


# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    # ğŸ‘‡ æ–°å¢ï¼šåˆ¤æ–­å€™é€‰è§£æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(final_output_path):
        print(f"ğŸ“„ å·²æ‰¾åˆ°å€™é€‰è§£æ–‡ä»¶ï¼š{final_output_path}")
        print("ç›´æ¥è·³è¿‡ç”Ÿæˆæ­¥éª¤ï¼Œå¼€å§‹è¯„ä¼°...\n")
        # åŠ è½½æ•°æ®é›†ï¼ˆä»…ç”¨äºè®¡ç®—ç»“æœæ—¶æ˜¾ç¤ºä»»åŠ¡æ•°é‡ï¼Œä¸ç”¨åŠ è½½æ¨¡å‹ï¼‰
        dataset = load_dataset(dataset_name, split="test")
        total_tasks = len(dataset)
    else:
        # ğŸ‘‡ æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼Œæ‰æ‰§è¡ŒåŸæ¥çš„ã€ŒåŠ è½½æ¨¡å‹+ç”Ÿæˆå€™é€‰è§£ã€é€»è¾‘
        # 1. åŠ è½½æ¨¡å‹ã€Tokenizerå’Œæ•°æ®é›†
        tokenizer, model = load_model_and_tokenizer(model_path)
        dataset = load_dataset(dataset_name, split="test")  # HumanEval+æµ‹è¯•é›†
        print(f"Loaded dataset with {len(dataset)} tasks\n")

        # 2. ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡çš„å€™é€‰è§£ï¼ˆæ¯ä¸ªä»»åŠ¡ç”Ÿæˆåç«‹å³ä¿å­˜ï¼Œå¸¦é—®é¢˜æ ‡æ³¨ï¼‰
        max_candidates_per_task = max(k_values)
        total_tasks = len(dataset)

        for idx, data in enumerate(dataset):
            # æå–å½“å‰ä»»åŠ¡çš„å…³é”®ä¿¡æ¯ï¼ˆç”¨äºæ ‡æ³¨ï¼‰
            task_id = f"HumanEval/{idx}"
            problem_prompt = data["prompt"].strip()  # å®Œæ•´é—®é¢˜æè¿°ï¼ˆå‡½æ•°å®šä¹‰+æ³¨é‡Šï¼‰
            # æå–å‡½æ•°åä½œä¸ºä»»åŠ¡åï¼ˆæ›´æ˜“è¯»ï¼‰
            func_name = None
            for line in problem_prompt.split("\n"):
                if line.startswith("def "):
                    func_name = line.split("def ")[1].split("(")[0].strip()
                    break
            task_name = f"func_{func_name}" if func_name else f"task_{idx}"  # å¦‚ func_fibonacci

            print(f"=== Processing task {idx + 1}/{total_tasks} ===")
            print(f"Task ID: {task_id}")
            print(f"Task Name: {task_name}")
            print(f"Problem Preview:\n{problem_prompt[:300]}..." if len(
                problem_prompt) > 300 else f"Problem:\n{problem_prompt}")

            # ç”Ÿæˆå½“å‰ä»»åŠ¡çš„å€™é€‰è§£
            candidates = generate_candidates(
                tokenizer=tokenizer,
                model=model,
                prompt=problem_prompt,
                num_candidates=max_candidates_per_task,
                batch_size=batch_size
            )

            # æ„å»ºå½“å‰ä»»åŠ¡çš„å€™é€‰è§£åˆ—è¡¨ï¼ˆå¸¦é—®é¢˜æ ‡æ³¨ï¼‰
            task_candidates = []
            for cand_idx, candidate in enumerate(candidates):
                task_candidates.append({
                    "task_id": task_id,  # æ ‡å‡†ä»»åŠ¡IDï¼ˆå¦‚ HumanEval/0ï¼‰
                    "task_name": task_name,  # æ˜“è¯»ä»»åŠ¡åï¼ˆå¦‚ func_fibonacciï¼‰
                    "problem_description": problem_prompt,  # å®Œæ•´é—®é¢˜æè¿°ï¼ˆæ–¹ä¾¿è¿½æº¯ï¼‰
                    "candidate_id": f"{task_id}_candidate_{cand_idx}",  # å”¯ä¸€å€™é€‰è§£IDï¼ˆå¦‚ HumanEval/0_candidate_5ï¼‰
                    "candidate_index": cand_idx,  # å€™é€‰è§£åœ¨å½“å‰ä»»åŠ¡ä¸­çš„åºå·ï¼ˆ0-99ï¼‰
                    "solution": candidate  # æ¨¡å‹ç”Ÿæˆçš„è§£
                })

            # é€ä»»åŠ¡è¿½åŠ ä¿å­˜ï¼ˆå¸¦é—®é¢˜æ ‡æ³¨ï¼‰
            save_task_candidates(task_candidates, final_output_path)

    # ğŸ‘‡ è¯„ä¼°é€»è¾‘ä¸å˜ï¼ˆæ— è®ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œéƒ½ä¼šæ‰§è¡Œï¼‰
    print("\n" + "=" * 60)
    print("Starting pass@k evaluation...")
    print("=" * 60)

    results = calculate_pass_at_k(final_output_path, k_values)

    # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
    print("\n" + "=" * 50)
    print("Pass@k Results (HumanEval+)")
    print("=" * 50)
    for k in k_values:
        pass_k = results[f"pass@{k}"]
        print(f"pass@{k}: {pass_k:.4f} ({int(pass_k * total_tasks)}/{total_tasks} tasks passed)")
    print("=" * 50)

    # ä¿å­˜pass@kç»“æœåˆ°åŒä¸€æ–‡ä»¶å¤¹
    results_path = os.path.join(output_dir, "pass_at_k_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\n Pass@k results saved to: {results_path}")
    print(f" å¸¦é—®é¢˜æ ‡æ³¨çš„å€™é€‰è§£æ–‡ä»¶ï¼š{final_output_path}")
    print(f" æ‰€æœ‰æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼š{output_dir}")